# ForgeServe
Simplify LLM Inference Deployment on Kubernetes. Declaratively configure and manage vLLM, TGI, Ollama, ... etc servers with this open-source CLI &amp; SDK.
